{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification for Support Tickets\n",
    "\n",
    "This notebook demonstrates text classification using traditional NLP techniques with scikit-learn. We'll implement both Naive Bayes and SVM classifiers to predict ticket types from text content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path to import local modules\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Import local modules\n",
    "from src.models import (\n",
    "    create_naive_bayes_pipeline, create_svm_pipeline, \n",
    "    get_grid_search_params, create_grid_search, train_ml_model, predict\n",
    ")\n",
    "from src.evaluate_model import (\n",
    "    evaluate_model, print_confusion_matrix, print_classification_report,\n",
    "    plot_confusion_matrix, plot_class_distribution, compare_models\n",
    ")\n",
    "from src.utils import save_model, load_model, save_config, load_config\n",
    "from src.config import get_config, update_config\n",
    "\n",
    "# For Jupyter Notebook\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default configuration\n",
    "config = get_config()\n",
    "\n",
    "# Update with our specific settings\n",
    "config = update_config(config, \n",
    "    data={\n",
    "        'train_path': '../data/ticket_train.csv',\n",
    "        'valid_path': '../data/ticket_valid.csv',\n",
    "        'test_path': '../data/ticket_test.csv',\n",
    "        'text_column': 'body',  # Column containing the text\n",
    "        'label_column': 'ticket_type'  # Column to predict\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the configuration\n",
    "from pprint import pprint\n",
    "print(\"Configuration:\")\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data from separate files\n",
    "train_data, train_labels, _ = load_data_ml(\n",
    "    config['data']['train_path'],\n",
    "    config['data']['text_column'],\n",
    "    config['data']['label_column']\n",
    ")\n",
    "\n",
    "test_data, test_labels, unique_labels = load_data_ml(\n",
    "    config['data']['test_path'],\n",
    "    config['data']['text_column'],\n",
    "    config['data']['label_column']\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Testing examples: {len(test_data)}\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")\n",
    "print(f\"Classes: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer \n",
    "count_vect = create_vectorizer()\n",
    "\n",
    "# Create TF-IDF transformer\n",
    "tfidf_transformer = create_tfidf_transformer()\n",
    "\n",
    "# Apply to training data\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "print(f\"Vectorized data shape: {vectorized_data.shape}\")\n",
    "\n",
    "features = tfidf_transformer.fit_transform(vectorized_data)\n",
    "print(f\"TF-IDF features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Naive Bayes pipeline\n",
    "nb_pipeline = create_naive_bayes_pipeline(\n",
    "    count_vect,\n",
    "    tfidf_transformer,\n",
    "    config['training']['fit_prior']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Naive Bayes model...\")\n",
    "nb_pipeline = train_ml_model(nb_pipeline, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "nb_predictions = predict(nb_pipeline, test_data)\n",
    "nb_accuracy = evaluate_model(nb_predictions, test_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print_confusion_matrix(test_labels, nb_predictions)\n",
    "\n",
    "# Print classification report\n",
    "print_classification_report(test_labels, nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(test_labels, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['training']['use_grid_search']:\n",
    "    print(\"Performing grid search for Naive Bayes...\")\n",
    "    \n",
    "    # Get grid search parameters\n",
    "    nb_params = get_grid_search_params('NB')\n",
    "    \n",
    "    # Create grid search\n",
    "    nb_gs = create_grid_search(\n",
    "        nb_pipeline,\n",
    "        nb_params,\n",
    "        config['training']['grid_search_jobs'],\n",
    "        config['training']['grid_search_cv']\n",
    "    )\n",
    "    \n",
    "    # Train the model with grid search\n",
    "    nb_gs = train_ml_model(nb_gs, train_data, train_labels)\n",
    "    \n",
    "    # Get best parameters\n",
    "    print(f\"Best parameters: {nb_gs.best_params_}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    nb_gs_predictions = predict(nb_gs, test_data)\n",
    "    nb_gs_accuracy = evaluate_model(nb_gs_predictions, test_labels)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print_confusion_matrix(test_labels, nb_gs_predictions)\n",
    "    \n",
    "    # Print classification report\n",
    "    print_classification_report(test_labels, nb_gs_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(test_labels, nb_gs_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVM pipeline\n",
    "svm_pipeline = create_svm_pipeline(\n",
    "    count_vect,\n",
    "    tfidf_transformer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training SVM model...\")\n",
    "svm_pipeline = train_ml_model(svm_pipeline, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "svm_predictions = predict(svm_pipeline, test_data)\n",
    "svm_accuracy = evaluate_model(svm_predictions, test_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print_confusion_matrix(test_labels, svm_predictions)\n",
    "\n",
    "# Print classification report\n",
    "print_classification_report(test_labels, svm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(test_labels, svm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['training']['use_grid_search']:\n",
    "    print(\"Performing grid search for SVM...\")\n",
    "    \n",
    "    # Get grid search parameters\n",
    "    svm_params = get_grid_search_params('SVM')\n",
    "    \n",
    "    # Create grid search\n",
    "    svm_gs = create_grid_search(\n",
    "        svm_pipeline,\n",
    "        svm_params,\n",
    "        config['training']['grid_search_jobs'],\n",
    "        config['training']['grid_search_cv']\n",
    "    )\n",
    "    \n",
    "    # Train the model with grid search\n",
    "    svm_gs = train_ml_model(svm_gs, train_data, train_labels)\n",
    "    \n",
    "    # Get best parameters\n",
    "    print(f\"Best parameters: {svm_gs.best_params_}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    svm_gs_predictions = predict(svm_gs, test_data)\n",
    "    svm_gs_accuracy = evaluate_model(svm_gs_predictions, test_labels)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print_confusion_matrix(test_labels, svm_gs_predictions)\n",
    "    \n",
    "    # Print classification report\n",
    "    print_classification_report(test_labels, svm_gs_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(test_labels, svm_gs_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of model names and accuracies\n",
    "model_names = ['Naive Bayes']\n",
    "accuracies = [nb_accuracy]\n",
    "\n",
    "if config['training']['use_grid_search']:\n",
    "    model_names.append('Naive Bayes (Grid Search)')\n",
    "    accuracies.append(nb_gs_accuracy)\n",
    "\n",
    "model_names.append('SVM')\n",
    "accuracies.append(svm_accuracy)\n",
    "\n",
    "if config['training']['use_grid_search']:\n",
    "    model_names.append('SVM (Grid Search)')\n",
    "    accuracies.append(svm_gs_accuracy)\n",
    "\n",
    "# Compare models\n",
    "compare_models(model_names, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['model']['save_model']:\n",
    "    # Determine the best model based on accuracy\n",
    "    best_model_idx = np.argmax(accuracies)\n",
    "    best_model_name = model_names[best_model_idx]\n",
    "    print(f\"Best model: {best_model_name} with accuracy {accuracies[best_model_idx]:.4f}\")\n",
    "    \n",
    "    # Get the best model\n",
    "    if best_model_name == 'Naive Bayes':\n",
    "        best_model = nb_pipeline\n",
    "    elif best_model_name == 'Naive Bayes (Grid Search)':\n",
    "        best_model = nb_gs\n",
    "    elif best_model_name == 'SVM':\n",
    "        best_model = svm_pipeline\n",
    "    elif best_model_name == 'SVM (Grid Search)':\n",
    "        best_model = svm_gs\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(config['data']['model_dir'], config['model']['model_name'])\n",
    "    save_model(best_model, model_path)\n",
    "    \n",
    "    # Save the configuration\n",
    "    if config['model']['save_config']:\n",
    "        config_path = os.path.join(config['data']['model_dir'], config['model']['config_name'])\n",
    "        save_config(config, config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions on New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict on new text\n",
    "def predict_on_text(model, text):\n",
    "    prediction = model.predict([text])[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted class: {prediction}\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on new texts\n",
    "test_texts = [\n",
    "    \"I am having issues with my email. It's not sending messages.\",\n",
    "    \"The software crashes every time I try to save my work.\",\n",
    "    \"Can you help me understand how to use the new feature?\",\n",
    "    \"I need to reset my password for the system access.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    predict_on_text(best_model, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADNE_textclf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
