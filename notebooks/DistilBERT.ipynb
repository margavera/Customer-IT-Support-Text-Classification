{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer IT Support Text Classification with DistilBERT\n",
    "\n",
    "This notebook demonstrates how to use DistilBERT for text classification of customer IT support tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path to import local modules\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Initialize TensorFlow correctly to avoid import conflicts\n",
    "from src.tf_init import tf, keras\n",
    "\n",
    "# Use GPU if available and set memory growth to avoid OOM errors\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to true for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Or limit memory usage\n",
    "        # tf.config.experimental.set_virtual_device_configuration(\n",
    "        #     gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "        print(f\"GPUs available: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error configuring GPUs: {e}\")\n",
    "\n",
    "# Regular imports after TensorFlow is initialized\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import the DistilBERT implementation after TensorFlow is properly initialized\n",
    "from src.model import DistilBertClassifier, save_model, load_model, model_predict\n",
    "from src.utils import encode_texts, encode_labels, load_training_conf\n",
    "from src.train import training_data, define_callbacks, train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Data Loading\n",
    "\n",
    "Let's start by loading our configuration and preprocessing our data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration - if no config file exists, default values will be used\n",
    "conf = load_training_conf()\n",
    "print(\"Configuration loaded:\")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract configuration values\n",
    "conf_train, conf_data = conf[\"training\"], conf[\"data\"]\n",
    "\n",
    "# Update dataset path if needed - use your actual file path\n",
    "# For CSV files with semicolon or comma separator (';' or ',')\n",
    "dataset_path = \"../data/dataset_filtered_tickets_english.csv\"  \n",
    "conf_data[\"dataset_path\"] = dataset_path\n",
    "\n",
    "# Set smaller max sequence length to reduce memory usage\n",
    "conf_data[\"max_words_per_message\"] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training and testing - wrapped in try-except for better error handling\n",
    "try:\n",
    "    (x_train, x_test, y_train, y_test), tokenizer, unique_labels = training_data(\n",
    "        conf_data[\"dataset_path\"],\n",
    "        conf_data[\"text_column\"],\n",
    "        conf_data[\"label_column\"],\n",
    "        test_size=conf_train.get(\"test_set_size\", 0.2),\n",
    "        subset_size=-1,  # Use all data\n",
    "        max_length=conf_data[\"max_words_per_message\"],\n",
    "        pad_to_max_length=conf_data.get(\"pad_to_max_length\", True),\n",
    "    )\n",
    "    \n",
    "    # Display information about our dataset\n",
    "    print(f\"Number of classes: {len(unique_labels)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    print(f\"Training samples: {len(y_train)}\")\n",
    "    print(f\"Test samples: {len(y_test)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "Now we'll initialize and train our DistilBERT model with memory-efficient settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DistilBERT model\n",
    "try:\n",
    "    # Clear memory before model creation\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = DistilBertClassifier(\n",
    "        num_labels=y_train.shape[1],\n",
    "        learning_rate=conf_train.get(\"learning_rate\", 5e-5),\n",
    "        dropout_rate=0.2,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    print(\"Model created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training\n",
    "callbacks = define_callbacks(\n",
    "    patience=conf_train.get(\"early_stopping_patience\", 2),\n",
    "    min_delta=conf_train.get(\"early_stopping_min_delta_acc\", 0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller batch sizes to avoid OOM errors\n",
    "BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "\n",
    "# Train the model with memory-efficient settings\n",
    "try:\n",
    "    test_loss, test_accuracy = train_model(\n",
    "        model,\n",
    "        x_train,\n",
    "        x_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        epochs=conf_train.get(\"epochs\", 3),\n",
    "        batch_size=BATCH_SIZE,  \n",
    "        eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance on the test set using batched prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the test set using batched prediction\n",
    "try:\n",
    "    # Clear memory before prediction\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Use batched prediction to avoid OOM errors\n",
    "    y_pred_logits = model.predict_in_batches(x_test, batch_size=8)\n",
    "    y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_true, y_pred, target_names=unique_labels))\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "try:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting confusion matrix: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the Model\n",
    "\n",
    "Now we'll save our trained model and tokenizer for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "try:\n",
    "    model_folder = \"../models\"  # Save one level up from notebooks\n",
    "    save_model(model, tokenizer, model_folder=model_folder)\n",
    "    \n",
    "    # Save unique labels for prediction\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    with open(f\"{model_folder}/unique_labels.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(unique_labels))\n",
    "        \n",
    "    print(f\"Model, tokenizer, and labels saved to {model_folder}/ directory\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make Predictions on New Examples\n",
    "\n",
    "Let's use our model to make some predictions on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on some examples\n",
    "try:\n",
    "    # Clear memory before running predictions\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    test_examples = [\n",
    "        \"I need help with my email account, I can't login\",\n",
    "        \"My server is down and customers can't access our website\",\n",
    "        \"I would like to request a refund for the software purchase\"\n",
    "    ]\n",
    "    \n",
    "    # Get predictions using small batch size\n",
    "    pred_indices = model_predict(model, tokenizer, test_examples, batch_size=1)\n",
    "    predictions = [unique_labels[idx] for idx in pred_indices]\n",
    "    \n",
    "    # Display results\n",
    "    for example, prediction in zip(test_examples, predictions):\n",
    "        print(f\"Text: {example}\\nPredicted queue: {prediction}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}